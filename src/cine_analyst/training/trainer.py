import os
import sys
import types
import torch
from loguru import logger
from cine_analyst.common.config import settings

# --- Unsloth/Torch í˜¸í™˜ì„± íŒ¨ì¹˜ ---
if not hasattr(torch, "int1"):
    torch.int1 = torch.bool 

if not hasattr(torch, "_inductor"):
    torch._inductor = types.ModuleType("_inductor")
if not hasattr(torch._inductor, "config"):
    torch._inductor.config = types.ModuleType("config")
sys.modules["torch._inductor.config"] = torch._inductor.config
# --- íŒ¨ì¹˜ ë ---

def train_model(
    base_model: str = settings.BASE_MODEL_NAME,
    data_path: str = settings.PROCESSED_DATA_PATH,
    output_dir: str = settings.MODEL_SAVE_DIR,
    max_steps: int = 60
):
    try:
        from unsloth import FastLanguageModel
        from trl import SFTTrainer
        from transformers import TrainingArguments
        from datasets import load_dataset
        # import wandb  # ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í•„ìš” ì‹œì—ë§Œ ì‚¬ìš©
    except ImportError:
        logger.error("âŒ 'unsloth' or 'trl' not found. Run 'poetry install --with train'")
        return

    # [ìˆ˜ì •] ìˆ˜ë™ wandb.init() ëŒ€ì‹  í™˜ê²½ ë³€ìˆ˜ë¡œ í”„ë¡œì íŠ¸ ì„¤ì •
    os.environ["WANDB_PROJECT"] = "cine-analyst-enterprise"
    # os.environ["WANDB_LOG_MODEL"] = "checkpoint" # í•„ìš” ì‹œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì—…ë¡œë“œ í™œì„±í™”

    logger.info(f"ğŸš€ Loading Base Model: {base_model}")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=base_model,
        max_seq_length=settings.MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=settings.LOAD_IN_4BIT,
    )

    # LoRA ì„¤ì • ì ìš©
    model = FastLanguageModel.get_peft_model(
        model,
        r=settings.LORA_RANK,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_alpha=settings.LORA_ALPHA,
        lora_dropout=settings.LORA_DROPOUT,
        bias="none",
        use_gradient_checkpointing="unsloth", 
        random_state=3407,
    )

    # ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• 
    raw_dataset = load_dataset("json", data_files=data_path, split="train")
    dataset_split = raw_dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = dataset_split["train"]
    eval_dataset = dataset_split["test"]

    def formatting_prompts_func(examples):
        convos = examples["messages"]
        texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]
        return {"text": texts}

    train_dataset = train_dataset.map(formatting_prompts_func, batched=True)
    eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)

    logger.info(f"ğŸ”¥ Starting SFT Training (Eval included)...")
    
    # Trainer ì„¤ì •
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field="text",
        max_seq_length=settings.MAX_SEQ_LENGTH,
        dataset_num_proc=2,
        packing=False,
        args=TrainingArguments(
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=5,
            max_steps=max_steps,
            learning_rate=2e-4,
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=1,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="outputs",
            report_to="wandb",          # [ì¤‘ìš”] Trainerê°€ ì§ì ‘ wandb ì„¸ì…˜ì„ ê´€ë¦¬í•¨
            run_name=f"train-{settings.ENV}-{max_steps}steps",
            eval_strategy="steps",
            eval_steps=10,
            save_strategy="steps",
            save_steps=10,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
        ),
    )

    # í•™ìŠµ ì‹œì‘
    trainer.train()
    
    # [ìˆ˜ì •] í•™ìŠµ ì¢…ë£Œ í›„ ë³„ë„ì˜ evaluate() í˜¸ì¶œì€ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°í•˜ê±°ë‚˜, 
    # í•„ìš”í•œ ê²½ìš° Trainerì˜ ì„¸ì…˜ì´ ìœ ì§€ë˜ëŠ” ë™ì•ˆ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    # ì´ë¯¸ load_best_model_at_endë¡œ ìµœì  ëª¨ë¸ì´ ë¡œë“œëœ ìƒíƒœì…ë‹ˆë‹¤.

    logger.info(f"ğŸ’¾ Saving best adapter to {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # [ìˆ˜ì •] wandb.finish() ì‚­ì œ (Trainerê°€ ìë™ìœ¼ë¡œ ì¢…ë£Œ ì²˜ë¦¬)

def run_cli():
    train_model()